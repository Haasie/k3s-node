apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: sabnzbd
  namespace: media
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 1.2.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    image:
      repository: ghcr.io/onedr0p/sabnzbd
      tag: 3.7.1@sha256:73d893b97571bbd699a495061b52959fa38b1dc37c54941c4ebfa400ec24c9e7
      pullPolicy: IfNotPresent
    env:
      TZ: "${TIMEZONE}"
      SABNZBD__PORT: &port 8080
      SABNZBD__HOST_WHITELIST_ENTRIES: >-
        sabnzbd,
        sabnzbd.media,
        sabnzbd.media.svc,
        sabnzbd.media.svc.cluster,
        sabnzbd.media.svc.cluster.local,
        sabnzbd.${SECRET_DOMAIN}
#    envFrom:
#      - secretRef:
#          name: *app
    service:
      main:
        ports:
          http:
            port: *port
    podSecurityContext:
      runAsUser: 568
      runAsGroup: 568
      fsGroup: 568
    ingress:
      main:
        enabled: true
        ingressClassName: "nginx"
        annotations:
          hajimari.io/enable: "true"
          hajimari.io/icon: "download"
          cert-manager.io/cluster-issuer: "letsencrypt-production"
          nginx.ingress.kubernetes.io/whitelist-source-range: "10.0.0.0/8,172.16.0.0/12,192.168.0.0/16"
        hosts:
          - host: "sabnzbd.${SECRET_DOMAIN}"
            paths:
              - path: /
                pathType: Prefix
        tls:
          - hosts:
              - "sabnzbd.${SECRET_DOMAIN}"
            secretName: "sabnzbd-tls"
    persistence:
      config:
        enabled: true
        existingClaim: sabnzbd-config-v1
      media:
        enabled: true
        type: nfs
        server: "192.168.1.4"
        path: /mnt/Cache/downloads
        mountPath: /downloads
        readOnly: false
      incomplete:
        enabled: true
        type: emptyDir
#    nodeSelector:
#      node-role.kubernetes.io/worker: "true"
    resources:
      requests:
        cpu: 10m
        memory: 250Mi
      limits:
        memory: 8000Mi
